# Basic Agent Secured Baseline  
**Phase 1 â€“ Foundations of Secure Agentic AI Development**

Secure-by-design research agent built with CrewAI  
Demonstrates security hygiene applied **before** writing functional logic

<br>

<p align="center">
  <img src="https://img.shields.io/badge/Phase-1%20Foundations-blue?style=for-the-badge&logo=shield&logoColor=white" alt="Phase 1">
  <img src="https://img.shields.io/badge/Security%20First-orange?style=for-the-badge" alt="Security First">
  <img src="https://img.shields.io/badge/CrewAI-0.80+-blueviolet?style=for-the-badge&logo=python" alt="CrewAI">
  <img src="https://img.shields.io/badge/OWASP%20Agentic-2026-important?style=for-the-badge" alt="OWASP Agentic">
</p>

<br>

## ğŸ¯ Purpose of this repository

This is **not** a production research agent.

This is a **deliberately minimal, security-annotated baseline** that shows:

- how security thinking starts **before the first line of logic**
- how to apply basic agentic-specific security controls from day zero
- how easily a vanilla agent can be broken (and what small changes already help)

It serves as the **anchor artifact** of the "Agentic AI Security â€“ Security-by-Design" 8â€“12 week learning path.

<br>

## ğŸ” Security posture â€“ what was added vs vanilla CrewAI

| Control                              | Vanilla CrewAI | This baseline                          | Why it matters                                      |
|:-------------------------------------|:--------------:|:---------------------------------------|:----------------------------------------------------|
| Prompts stored in Git                | âŒ             | âœ… (prompts/ folder)                   | Prompts are **code** â†’ version, review, sign       |
| Structured audit logging (JSONL)     | âŒ (only console) | âœ… full event trail (thought/tool/output) | Mandatory for forensic reconstruction              |
| Tool allow-list                      | âŒ             | âœ… strict function in tools.py          | Prevents surprise tool registration                |
| Max iterations limit                 | âŒ             | âœ… (8 by default)                      | Basic protection against infinite loops            |
| No delegation by default             | âŒ             | âœ…                                     | Reduces privilege escalation surface               |
| .env + .gitignore hygiene            | partial        | âœ… strict                              | Prevents key leaks                                 |
| Attack demonstrations documented     | â€”              | âœ… three concrete attacks + logs        | Proves understanding of real agentic threats       |

<br>

## ğŸ›¡ï¸ Alignment with IBM Agentic AI Security Principles (2026)

| Principle                            | How it is addressed in this baseline                              |
|:-------------------------------------|:------------------------------------------------------------------|
| 1. Keep an eye on them               | Verbose logging + final report requires human review             |
| 2. Contain and compartmentalize      | Strict tool allow-list + no delegation + iteration limit         |
| 3. Remember the full ML lifecycle    | Prompts versioned â†’ can be hardened / red-teamed like code       |
| 4. Secure the action layer           | Only pre-approved tools can be called â†’ no dynamic tool discovery|

<br>

## âš”ï¸ Attacks performed & documented (Phase 1 lab requirement)

All attacks were run against **slightly modified versions** of the agent (removing one or more of the controls above) to demonstrate realistic failure modes.

| # | Attack name                        | OWASP ASI ref       | Success on vanilla? | Mitigated by baseline? | Folder                          |
|---|------------------------------------|---------------------|----------------------|------------------------|---------------------------------|
| 1 | Goal hijacking via prefix prompt   | ASI01               | Yes                  | Partial (role refusal) | `attacks/01_goal_hijack.md`     |
| 2 | Tool misuse â€“ attempted code exfil | ASI02               | Yes                  | Yes (no code exec tool)| `attacks/02_tool_misuse.md`     |
| 3 | Indirect prompt injection          | ASI02 / ASI07       | Yes                  | Partial                | `attacks/03_indirect_injection.md` |

Detailed evidence (prompts used, agent responses, log excerpts) â†’ see [`/attacks/`](./attacks) folder.

<br>

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    A[User Query] --> B[Researcher Agent]
    B -->|allowed only| C[Tavily Search Tool]
    C --> B
    B --> D[Reporting Analyst Agent]
    D --> E[Markdown Report]
    style B fill:#2d6a4f,stroke:#fff,stroke-width:2px
    style D fill:#2d6a4f,stroke:#fff,stroke-width:2px

ğŸ“‚ Project layout
basic-agent-secured-baseline/
â”œâ”€â”€ prompts/                    â† version-controlled system prompts (treated as code)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ secured_research_crew/
â”‚       â”œâ”€â”€ config/             â† agents.yaml / tasks.yaml skeletons
â”‚       â”œâ”€â”€ callbacks.py        â† structured JSONL security logger
â”‚       â”œâ”€â”€ tools.py            â† strict tool allow-list factory
â”‚       â”œâ”€â”€ crew.py
â”‚       â””â”€â”€ main.py
â”œâ”€â”€ attacks/                    â† documented attack attempts + evidence
â”œâ”€â”€ logs/runs/                  â† per-run structured audit trails (.jsonl)
â”œâ”€â”€ outputs/                    â† final reports
â”œâ”€â”€ .github/workflows/          â† basic CI stub
â”œâ”€â”€ requirements.txt
â””â”€â”€ security_baseline.md        â† detailed delta vs vanilla CrewAI
